---
description: Visualize PyTorch Lightning models with W&B
---

# PyTorch Lightning

PyTorch Lightning fournit un wrapper l√©ger pour organiser votre code PyTorch et facilement ajouter des caract√©ristiques avanc√©es comme [l‚Äôentra√Ænement distribu√©](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html) ou la [pr√©cision 16-bit](https://pytorch-lightning.readthedocs.io/en/latest/amp.html). W&B fournit un wrapper l√©ger pour enregistrer vos exp√©riences d‚Äôapprentissage automatique. Nous sommes incorpor√©s directement depuis la librairie de PyTorch Lightning, et vous pouvez toujours vous r√©f√©rer √† [leur documentation](https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#weights-and-biases). 

## ‚ö° Allez √† la vitesse de l‚Äô√©clair en deux lignes √† peine :

```python
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning import Trainer

wandb_logger = WandbLogger()
trainer = Trainer(logger=wandb_logger)
```

## ‚úÖ Consultez de vrais exemples !

Nous avons cr√©√© quelques exemples pour que vous puissiez voir comment fonctionne cette int√©gration :

*  [D√©mo dans Google Colab](https://colab.research.google.com/drive/16d1uctGaw2y9KhGBlINNTsWpmlXdJwRW?usp=sharing) avec optimisation d‚Äôhyperparam√®tres
* [Tutoriel ](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Supercharge_your_Training_with_Pytorch_Lightning_%2B_Weights_%26_Biases.ipynb): Boostez votre Entra√Ænement avec Pytorch Lightning + Weights & Biases
* [Segmentation s√©mantique avec Lightning ](https://app.wandb.ai/borisd13/lightning-kitti/reports/Lightning-Kitti--Vmlldzo3MTcyMw): optimiser les r√©seaux neuronaux pour les voitures autonomes
*  [Un guide pas-√†-pas](https://app.wandb.ai/cayush/pytorchlightning/reports/Use-Pytorch-Lightning-with-Weights-%26-Biases--Vmlldzo2NjQ1Mw) pour retracez les performances de votre mod√®le Lightning

## **üíª** R√©f√©rence API

### `WandbLogger`

Param√®tres optionnels :

* **name** \(_str_\) ‚Äì affiche le nom pour l‚Äôessai.
* **save\_dir** \(_str_\) ‚Äì chemin o√π les donn√©es sont enregistr√©es \(par d√©faut, dossier wandb\).
* **offline** \(_bool_\) ‚Äì ex√©cuter hors-ligne \(les donn√©es peuvent √™tre transmises plus tard aux serveurs wandb\)
* **id** \(_str_\) ‚Äì fixe la version, surtout utilis√© pour reprendre un essai pr√©c√©dent.
* **version** \(_str_\) ‚Äì m√™me chose que version \(legacy\).
* **anonymous** \(_bool_\) ‚Äì permet ou emp√™che explicitement l‚Äôenregistrement anonyme.
* **project** \(_str_\) ‚Äì le nom du projet auquel cet essai se rapportera.
* **log\_model** \(_bool_\) ‚Äì sauvegarde les checkpoints dans le wandb dir pour les envoyer aux serveurs W&B.
* **prefix** \(_str_\) ‚Äì cha√Æne √† mettre au d√©but des clefs de mesure.
* **sync\_step** \(_bool_\) - synchronise les √©tapes Trainer avec les √©tapes wandb \(par d√©faut, True\).
* **\*\*kwargs** ‚Äì des ****arguments additionnels comme `entity`, `group`, `tags`, etc. utilis√©s par wandb.init peuvent √™tre pass√©s comme arguments mots-clefs dans ce logger. 

### **`WandbLogger.watch`**

Enregistrez la topologie de votre mod√®le ainsi que des d√©grad√©s et des poids optionnels.

```python
wandb_logger.watch(model, log='gradients', log_freq=100)
```

Param√®tres :

* **model** \(_nn.Module_\) ‚Äì mod√®le √† enregistrer.
* **log** \(_str_\) ‚Äì peut √™tre "gradients" \(d√©grad√©s, par d√©faut\), "parameters" \(param√®tres\), "all" \(tout\) ou None \(Aucun\).
* **log\_freq** \(_int_\) ‚Äì compteur d‚Äô√©tape entre l‚Äôenregistrement des d√©grad√©s et des param√®tres \(par d√©faut, 100\).

### **`WandbLogger.log_hyperparams`**

Enregistrez la configuration d‚Äôhyperparam√®tres.

Note : cette fonction est automatiquement appel√©e lorsque _`LightningModule.save_hyperparameters()`_ est utilis√©e.

```python
wandb_logger.log_hyperparams(params)
```

 Param√®tres :

* **params** \(dict\)  ‚Äì dictionnaire avec les noms d‚Äôhyperparam√®tres en tant que clefs et les valeurs de configuration en tant que valeurs

### `WandbLogger.log_metrics`

Enregistrez les mesures d‚Äôentra√Ænement.

_Note : cette fonction est automatiquement appel√©e par   `LightningModule.log('metric', value)`_

```python
wandb_logger.log_metrics(metrics, step=None)
```

Param√®tres :

* **metric** \(numeric\) ‚Äì dictionnaire avec les noms de mesures en tant que clefs et les quantit√©s mesur√©es en tant que valeurs
* **step** \(int\|None\) ‚Äì nombre d‚Äô√©tapes auxquelles les mesures doivent √™tre enregistr√©es.

\*\*\*\*

